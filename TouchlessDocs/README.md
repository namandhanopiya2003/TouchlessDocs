# TouchlessDocs: Gesture-Operated Document Viewer for Field Workers

## ðŸ§  ABOUT THIS PROJECT ==>

- TouchlessDocs is a Python-based, AI-powered document viewer that lets field professionals (engineers, doctors, mechanics, etc.) **interact with PDF documents using hand gestures and voice commands** â€” no need to touch their devices! 

- This application uses:
**Rule-based gesture recognition** (no training required)
**Voice command intent classification using NLP**
**Real-time PDF rendering and annotation**
**User interaction logging for future predictive analytics**

- Ideal for hands-free access to manuals, blueprints, or medical reports in dusty, wet, or sterile environments.

---

## âš™ TECHNOLOGIES USED ==>

- **Python**
- **OpenCV**                                  (video streaming & image annotation)
- **MediaPipe**                               (hand landmark detection)
- **PyMuPDF (fitz)**                          (PDF rendering & manipulation)
- **SpeechRecognition**                       (speech-to-text)
- **NLTK / Simple NLP rules**                 (command intent classification)
- **Matplotlib / Pandas**                     (analytics, optional)
- **Pickle / CSV**                            (logging data)

---

## ðŸ“ PROJECT FOLDER STRUCTURE ==>

TouchlessDocs/
â”‚
â”œâ”€â”€ app/                      # Core application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py               # Entry point
â”‚   â”œâ”€â”€ gesture_control.py    # Rule-based gesture detection
â”‚   â”œâ”€â”€ voice_commands.py     # Voice-to-text + NLP intent recognition
â”‚   â”œâ”€â”€ pdf_viewer.py         # PDF viewing and interaction
â”‚   â”œâ”€â”€ annotator.py          # Drawing/annotation tool
â”‚   â””â”€â”€ session_logger.py     # User interaction logging and analytics
â”‚
â”œâ”€â”€ utils/                    # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mediapipe_utils.py    # MediaPipe initialization and helpers
â”‚   â”œâ”€â”€ nlp_utils.py          # Intent classification
â”‚   â””â”€â”€ gesture_rules.py      # Reusable rule functions
â”‚
â”œâ”€â”€ data/                     # Logs, voice samples, interaction data
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ analytics/
â”‚
â”œâ”€â”€ models/                   # Optional future AI models (e.g. ChatGPT, user modeling)
â”‚   â””â”€â”€ README.md             # Explain how to add/plug in AI models here
â”‚
â”œâ”€â”€ assets/                   # Icons, images, example PDFs
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ run.py                    # Launch the app

---

## ðŸ“ WHAT EACH FILE DOES ==>

- **`app/gestures.py`**  
  Implements gesture detection rules using MediaPipe landmarks. Detects gestures like "next", "previous", and "pinch".

- **`app/pdf_viewer.py`**  
  Handles PDF rendering, page navigation, and annotation using PyMuPDF.

- **`app/voice_control.py`**  
  Uses SpeechRecognition to listen for voice input and classify it using rule-based NLP.

- **`app/nlp_utils.py`**  
  Contains logic for matching spoken phrases to commands like `"next_page"` or `"save_comment"`.

- **`app/analytics.py`**  
  Logs gesture/voice actions to CSV and computes basic usage stats.

- **`app/main.py`**  
  Initializes the camera, gesture detection, voice command handling, and PDF rendering.

- **`run.py`**  
  The main launcher script for running the application.

---

## ðŸš€ HOW TO RUN ==>

# Step 1: Open CMD and move to the project directory
cd "D:\TouchlessDocs"
D:

# Step 2: Create and activate a virtual environment
python -m venv venv
venv\\Scripts\\activate

# Step 3: Install dependencies
pip install -r requirements.txt

# Step 4: Make sure there's a PDF in the assets/ folder
Rename PDF to example.pdf or update the path in main.py

# Step 5: Run the app
python run.py

# Step 6: Controls inside the app
- Show hand gesture for next/previous page.
- Press v to speak a command like "next page", "save", "zoom in", etc.
- Press q to quit.

---

âœ¨ SAMPLE OUTPUT ==>

ðŸ“„ Document: example.pdf
ðŸŽ¥ Webcam: Activated
ðŸ–ï¸ Detected Gesture: "Next Page"
ðŸ—£ï¸ Voice Command: "Save comment" â†’ save
ðŸ’¾ Screenshot saved to screenshots/gesture_2025_06_13_111212.jpg
ðŸ“Š Analytics: 3 gestures, 2 voice commands logged in session.

---

ðŸ“¬ CONTACT ==>
For queries, feature suggestions, or collaboration, feel free to connect!


---
