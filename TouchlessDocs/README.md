# TouchlessDocs: Gesture-Operated Document Viewer for Field Workers

## ğŸ§  ABOUT THIS PROJECT ==>

- TouchlessDocs is a Python-based, AI-powered document viewer that lets field professionals (engineers, doctors, mechanics, etc.) **interact with PDF documents using hand gestures and voice commands** â€” no need to touch their devices! 

- This application uses:
**Rule-based gesture recognition** (no training required)
**Voice command intent classification using NLP**
**Real-time PDF rendering and annotation**
**User interaction logging for future predictive analytics**

- Ideal for hands-free access to manuals, blueprints, or medical reports in dusty, wet, or sterile environments.

---

## âš™ TECHNOLOGIES USED ==>

- **Python**
- **OpenCV**                                  (video streaming & image annotation)
- **MediaPipe**                               (hand landmark detection)
- **PyMuPDF (fitz)**                          (PDF rendering & manipulation)
- **SpeechRecognition**                       (speech-to-text)
- **NLTK / Simple NLP rules**                 (command intent classification)
- **Matplotlib / Pandas**                     (analytics, optional)
- **Pickle / CSV**                            (logging data)

---

## ğŸ“ PROJECT FOLDER STRUCTURE ==>

TouchlessDocs/<br>
â”‚<br>
â”œâ”€â”€ app/                      # Core application<br>
â”‚   â”œâ”€â”€ __init__.py<br>
â”‚   â”œâ”€â”€ main.py               # Entry point<br>
â”‚   â”œâ”€â”€ gesture_control.py    # Rule-based gesture detection<br>
â”‚   â”œâ”€â”€ voice_commands.py     # Voice-to-text + NLP intent recognition<br>
â”‚   â”œâ”€â”€ pdf_viewer.py         # PDF viewing and interaction<br>
â”‚   â”œâ”€â”€ annotator.py          # Drawing/annotation tool<br>
â”‚   â””â”€â”€ session_logger.py     # User interaction logging and analytics<br>
â”‚<br>
â”œâ”€â”€ utils/                    # Utility modules<br>
â”‚   â”œâ”€â”€ __init__.py<br>
â”‚   â”œâ”€â”€ mediapipe_utils.py    # MediaPipe initialization and helpers<br>
â”‚   â”œâ”€â”€ nlp_utils.py          # Intent classification<br>
â”‚   â””â”€â”€ gesture_rules.py      # Reusable rule functions<br>
â”‚<br>
â”œâ”€â”€ data/                     # Logs, voice samples, interaction data<br>
â”‚   â”œâ”€â”€ logs/<br>
â”‚   â””â”€â”€ analytics/<br>
â”‚<br>
â”œâ”€â”€ models/                   # Optional future AI models (e.g. ChatGPT, user modeling)<br>
â”‚   â””â”€â”€ README.md             # Explain how to add/plug in AI models here<br>
â”‚<br>
â”œâ”€â”€ assets/                   # Icons, images, example PDFs<br>
â”‚<br>
â”œâ”€â”€ requirements.txt<br>
â”œâ”€â”€ README.md<br>
â””â”€â”€ run.py                    # Launch the app

---

## ğŸ“ WHAT EACH FILE DOES ==>

- **`app/gestures.py`**  
  Implements gesture detection rules using MediaPipe landmarks. Detects gestures like "next", "previous", and "pinch".

- **`app/pdf_viewer.py`**  
  Handles PDF rendering, page navigation, and annotation using PyMuPDF.

- **`app/voice_control.py`**  
  Uses SpeechRecognition to listen for voice input and classify it using rule-based NLP.

- **`app/nlp_utils.py`**  
  Contains logic for matching spoken phrases to commands like `"next_page"` or `"save_comment"`.

- **`app/analytics.py`**  
  Logs gesture/voice actions to CSV and computes basic usage stats.

- **`app/main.py`**  
  Initializes the camera, gesture detection, voice command handling, and PDF rendering.

- **`run.py`**  
  The main launcher script for running the application.

---

## ğŸš€ HOW TO RUN ==>

# Step 1: Open CMD and move to the project directory
cd "D:\TouchlessDocs"
D:

# Step 2: Create and activate a virtual environment
python -m venv venv
venv\\Scripts\\activate

# Step 3: Install dependencies
pip install -r requirements.txt

# Step 4: Make sure there's a PDF in the assets/ folder
Rename PDF to example.pdf or update the path in main.py

# Step 5: Run the app
python run.py

# Step 6: Controls inside the app
- Show hand gesture for next/previous page.
- Press v to speak a command like "next page", "save", "zoom in", etc.
- Press q to quit.

---

âœ¨ SAMPLE OUTPUT ==>

ğŸ“„ Document: example.pdf<br>
ğŸ¥ Webcam: Activated<br>
ğŸ–ï¸ Detected Gesture: "Next Page"<br>
ğŸ—£ï¸ Voice Command: "Save comment" â†’ save<br>
ğŸ’¾ Screenshot saved to screenshots/gesture_2025_06_13_111212.jpg<br>
ğŸ“Š Analytics: 3 gestures, 2 voice commands logged in session.

---

ğŸ“¬ CONTACT ==>
For queries, feature suggestions, or collaboration, feel free to connect!

---


